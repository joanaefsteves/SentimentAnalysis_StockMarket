{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562bdc15",
   "metadata": {},
   "source": [
    "# Test classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f82c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "data_folder = os.path.join(os.path.dirname(os.getcwd()), 'data')\n",
    "data_train = pd.read_csv(os.path.join(data_folder, 'train.csv'))\n",
    "print(f\"Dataset shape: {data_train.shape}\")\n",
    "print(f\"Label distribution:\\n{data_train['label'].value_counts()}\")\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b8d5fc",
   "metadata": {},
   "source": [
    "## Setup and Import Classification Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f21fbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: \n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "os.chdir(os.path.dirname(os.getcwd()))\n",
    "print(\"Changed working directory to:\", os.getcwd())\n",
    "\n",
    "from src.classification import SklearnSentimentClassifier, KerasSentimentClassifier\n",
    "from src.embedding import TextEmbedder\n",
    "from src.preprocessing import Preprocessing\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print(\"Modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee8ac7",
   "metadata": {},
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a larger subset for classification testing\n",
    "subset_size = 2000\n",
    "sample_data = data_train.sample(n=subset_size, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_data['text'], sample_data['label'], \n",
    "    test_size=0.2, stratify=sample_data['label'], random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training label distribution:\\n{pd.Series(y_train).value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19145786",
   "metadata": {},
   "source": [
    "## Generate Embeddings for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12dd149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the best performing embedding from your previous analysis\n",
    "print(\"Generating embeddings...\")\n",
    "\n",
    "# Use SentenceTransformer for high-quality embeddings\n",
    "embedder = TextEmbedder(method='transformer', model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "embedder.fit(X_train.tolist())\n",
    "\n",
    "X_train_embeddings = embedder.transform(X_train.tolist())\n",
    "X_test_embeddings = embedder.transform(X_test.tolist())\n",
    "\n",
    "print(f\"Embedding shape: {X_train_embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {X_train_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9471cf2",
   "metadata": {},
   "source": [
    "### Balance Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a833660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE-Tomek for better class balance\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote_tomek.fit_resample(X_train_embeddings, y_train)\n",
    "\n",
    "print(f\"Original training set size: {len(X_train_embeddings)}\")\n",
    "print(f\"Balanced training set size: {len(X_train_balanced)}\")\n",
    "print(f\"Balanced label distribution:\\n{pd.Series(y_train_balanced).value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33342f",
   "metadata": {},
   "source": [
    "## Performance Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_configs = [\n",
    "    {\n",
    "        'name': 'Logistic Regression',\n",
    "        'model_type': 'logistic_regression',\n",
    "        'params': {'C': 1.0, 'max_iter': 1000}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Random Forest',\n",
    "        'model_type': 'random_forest',\n",
    "        'params': {'n_estimators': 100, 'max_depth': 10}\n",
    "    },\n",
    "    {\n",
    "        'name': 'SVM (RBF)',\n",
    "        'model_type': 'svm',\n",
    "        'params': {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale'}\n",
    "    },\n",
    "    {\n",
    "        'name': 'SVM (Linear)',\n",
    "        'model_type': 'svm',\n",
    "        'params': {'C': 1.0, 'kernel': 'linear'}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Naive Bayes',\n",
    "        'model_type': 'naive_bayes',\n",
    "        'params': {}\n",
    "    }\n",
    "]\n",
    "\n",
    "keras_configs = [\n",
    "    {\n",
    "        'name': 'Neural Network (Small)',\n",
    "        'params': {\n",
    "            'hidden_layers': [64, 32],\n",
    "            'dropout_rate': 0.3,\n",
    "            'learning_rate': 0.001}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Neural Network (Medium)',\n",
    "        'params': {\n",
    "            'hidden_layers': [128, 64, 32],\n",
    "            'dropout_rate': 0.4,\n",
    "            'learning_rate': 0.001\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Neural Network (Large)',\n",
    "        'params': {\n",
    "            'hidden_layers': [256, 128, 64, 32],\n",
    "            'dropout_rate': 0.5,\n",
    "            'learning_rate': 0.0001\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(sklearn_configs)} Sklearn classifiers\")\n",
    "print(f\"Configured {len(keras_configs)} Keras classifiers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ac87d",
   "metadata": {},
   "source": [
    "###  Sklearn Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_results = []\n",
    "sklearn_models = {}\n",
    "\n",
    "print(\" Testing Sklearn Classifiers...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for config in sklearn_configs:\n",
    "    print(f\"\\nTesting {config['name']}...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize classifier\n",
    "        classifier = SklearnSentimentClassifier(\n",
    "            model_type=config['model_type'], \n",
    "            **config['params']\n",
    "        )\n",
    "        \n",
    "        # Prepare labels\n",
    "        y_train_encoded = classifier.prepare_labels(pd.Series(y_train_balanced))\n",
    "        y_test_encoded = classifier.transform_labels(pd.Series(y_test))\n",
    "        \n",
    "        # Time training\n",
    "        start_time = time.time()\n",
    "        classifier.train(X_train_balanced, y_train_encoded)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Time prediction\n",
    "        start_time = time.time()\n",
    "        predictions = classifier.predict(X_test_embeddings)\n",
    "        predict_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate\n",
    "        results = classifier.evaluate(X_test_embeddings, y_test_encoded)\n",
    "        \n",
    "        # Cross-validate\n",
    "        cv_results = classifier.cross_validate(X_train_balanced, y_train_encoded, cv=5)\n",
    "        \n",
    "        # Store results\n",
    "        sklearn_results.append({\n",
    "            'Method': config['name'],\n",
    "            'Model Type': config['model_type'],\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'CV Mean': cv_results['mean_score'],\n",
    "            'CV Std': cv_results['std_score'],\n",
    "            'Train Time (s)': train_time,\n",
    "            'Predict Time (s)': predict_time,\n",
    "            'Total Time (s)': train_time + predict_time,\n",
    "            'Predictions/sec': len(X_test_embeddings) / predict_time\n",
    "        })\n",
    "        \n",
    "        # Store model for later analysis\n",
    "        sklearn_models[config['name']] = {\n",
    "            'classifier': classifier,\n",
    "            'predictions': predictions,\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        print(f\"    Success - Accuracy: {results['accuracy']:.3f}, CV: {cv_results['mean_score']:.3f} Â± {cv_results['std_score']:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    Failed: {str(e)}\")\n",
    "        sklearn_results.append({\n",
    "            'Method': config['name'],\n",
    "            'Error': str(e)\n",
    "        })\n",
    "\n",
    "# Create results DataFrame\n",
    "sklearn_df = pd.DataFrame(sklearn_results)\n",
    "print(\"\\nðŸ“Š Sklearn Classifier Results:\")\n",
    "print(sklearn_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77411c",
   "metadata": {},
   "source": [
    "### Keras Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd257b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_results = []\n",
    "keras_models = {}\n",
    "\n",
    "print(\"\\n Testing Keras Neural Networks...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for config in keras_configs:\n",
    "    print(f\"\\nTesting {config['name']}...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize classifier\n",
    "        classifier = KerasSentimentClassifier(**config['params'])\n",
    "        \n",
    "        # Prepare labels\n",
    "        y_train_encoded = classifier.prepare_labels(pd.Series(y_train_balanced))\n",
    "        y_test_encoded = classifier.transform_labels(pd.Series(y_test))\n",
    "        \n",
    "        # Split training data for validation\n",
    "        X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "            X_train_balanced, y_train_encoded, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Time training\n",
    "        start_time = time.time()\n",
    "        history = classifier.train(\n",
    "            X_train_split, y_train_split,\n",
    "            X_val=X_val_split, y_val=y_val_split,\n",
    "            epochs=50, batch_size=32, verbose=0\n",
    "        )\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Time prediction\n",
    "        start_time = time.time()\n",
    "        predictions = classifier.predict(X_test_embeddings)\n",
    "        predict_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate\n",
    "        results = classifier.evaluate(X_test_embeddings, y_test_encoded)\n",
    "        \n",
    "        # Store results\n",
    "        keras_results.append({\n",
    "            'Method': config['name'],\n",
    "            'Hidden Layers': str(config['params']['hidden_layers']),\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Test Loss': results['test_loss'],\n",
    "            'Train Time (s)': train_time,\n",
    "            'Predict Time (s)': predict_time,\n",
    "            'Total Time (s)': train_time + predict_time,\n",
    "            'Epochs Trained': len(history.history['loss']),\n",
    "            'Parameters': classifier.model.count_params()\n",
    "        })\n",
    "        \n",
    "        # Store model for later analysis\n",
    "        keras_models[config['name']] = {\n",
    "            'classifier': classifier,\n",
    "            'predictions': predictions,\n",
    "            'results': results,\n",
    "            'history': history\n",
    "        }\n",
    "        \n",
    "        print(f\"     Success - Accuracy: {results['accuracy']:.3f}, Loss: {results['test_loss']:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     Failed: {str(e)}\")\n",
    "        keras_results.append({\n",
    "            'Method': config['name'],\n",
    "            'Error': str(e)\n",
    "        })\n",
    "\n",
    "# Create results DataFrame\n",
    "keras_df = pd.DataFrame(keras_results)\n",
    "print(\"\\n Keras Classifier Results:\")\n",
    "print(keras_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ccafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb4e2d",
   "metadata": {},
   "source": [
    "## Performance Visualization with Additional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Debugging DataFrame structures:\")\n",
    "print(\"\\nSklearn DataFrame columns:\", sklearn_df.columns.tolist())\n",
    "print(\"Sklearn DataFrame shape:\", sklearn_df.shape)\n",
    "print(\"\\nKeras DataFrame columns:\", keras_df.columns.tolist())\n",
    "print(\"Keras DataFrame shape:\", keras_df.shape)\n",
    "\n",
    "\n",
    "# Combine successful results for comparison with proper error handling\n",
    "successful_sklearn = sklearn_df.dropna(subset=['Accuracy']) if 'Accuracy' in sklearn_df.columns else pd.DataFrame()\n",
    "successful_keras = keras_df.dropna(subset=['Accuracy']) if 'Accuracy' in keras_df.columns else pd.DataFrame()\n",
    "\n",
    "print(f\"\\n Successful models found:\")\n",
    "print(f\"   â€¢ Sklearn models: {len(successful_sklearn)}\")\n",
    "print(f\"   â€¢ Keras models: {len(successful_keras)}\")\n",
    "\n",
    "if len(successful_sklearn) > 0 or len(successful_keras) > 0:\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Accuracy comparison with value labels\n",
    "    all_methods = []\n",
    "    all_accuracies = []\n",
    "    all_colors = []\n",
    "    \n",
    "    if len(successful_sklearn) > 0:\n",
    "        all_methods.extend(successful_sklearn['Method'])\n",
    "        all_accuracies.extend(successful_sklearn['Accuracy'])\n",
    "        all_colors.extend(['lightcoral'] * len(successful_sklearn))\n",
    "    \n",
    "    if len(successful_keras) > 0:\n",
    "        all_methods.extend(successful_keras['Method'])\n",
    "        all_accuracies.extend(successful_keras['Accuracy'])\n",
    "        all_colors.extend(['lightblue'] * len(successful_keras))\n",
    "    \n",
    "    if len(all_methods) > 0:\n",
    "        bars1 = ax1.bar(range(len(all_methods)), all_accuracies, color=all_colors)\n",
    "        ax1.set_title('Classification Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "        ax1.set_xticks(range(len(all_methods)))\n",
    "        ax1.set_xticklabels(all_methods, rotation=45, ha='right')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylim(0, 1.0)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars1, all_accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax1.annotate(f'{acc:.3f}',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'No Models\\nAvailable', \n",
    "                ha='center', va='center', transform=ax1.transAxes, fontsize=14)\n",
    "        ax1.set_title('Classification Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Training time comparison with log scale if needed\n",
    "    all_train_times = []\n",
    "    if len(successful_sklearn) > 0 and 'Train Time (s)' in successful_sklearn.columns:\n",
    "        all_train_times.extend(successful_sklearn['Train Time (s)'])\n",
    "    if len(successful_keras) > 0 and 'Train Time (s)' in successful_keras.columns:\n",
    "        all_train_times.extend(successful_keras['Train Time (s)'])\n",
    "    \n",
    "    if len(all_train_times) > 0:\n",
    "        bars2 = ax2.bar(range(len(all_methods)), all_train_times, color=all_colors)\n",
    "        ax2.set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylabel('Time (seconds)', fontsize=12)\n",
    "        ax2.set_xticks(range(len(all_methods)))\n",
    "        ax2.set_xticklabels(all_methods, rotation=45, ha='right')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Use log scale if there's a large difference in training times\n",
    "        time_ratio = max(all_train_times) / min(all_train_times) if min(all_train_times) > 0 else 1\n",
    "        if time_ratio > 10:\n",
    "            ax2.set_yscale('log')\n",
    "            ax2.set_ylabel('Time (seconds) - Log Scale', fontsize=12)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, time_val in zip(bars2, all_train_times):\n",
    "            height = bar.get_height()\n",
    "            ax2.annotate(f'{time_val:.2f}s',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No Training Time\\nData Available', \n",
    "                ha='center', va='center', transform=ax2.transAxes, fontsize=14)\n",
    "        ax2.set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Cross-validation scores (Sklearn only) with enhanced styling\n",
    "    if len(successful_sklearn) > 0 and 'CV Mean' in successful_sklearn.columns:\n",
    "        bars3 = ax3.bar(successful_sklearn['Method'], successful_sklearn['CV Mean'], \n",
    "                       yerr=successful_sklearn['CV Std'] if 'CV Std' in successful_sklearn.columns else None, \n",
    "                       color='lightcoral', alpha=0.7, \n",
    "                       capsize=5, error_kw={'linewidth': 2, 'markeredgewidth': 2})\n",
    "        ax3.set_title('Cross-Validation Scores (Sklearn Models)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_ylabel('CV Mean Accuracy', fontsize=12)\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_ylim(0, 1.0)\n",
    "        \n",
    "        # Add value labels\n",
    "        if 'CV Std' in successful_sklearn.columns:\n",
    "            for i, (mean_val, std_val) in enumerate(zip(successful_sklearn['CV Mean'], successful_sklearn['CV Std'])):\n",
    "                ax3.annotate(f'{mean_val:.3f}Â±{std_val:.3f}',\n",
    "                            xy=(i, mean_val + std_val),\n",
    "                            xytext=(0, 3),\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom', fontsize=10)\n",
    "        else:\n",
    "            for i, mean_val in enumerate(successful_sklearn['CV Mean']):\n",
    "                ax3.annotate(f'{mean_val:.3f}',\n",
    "                            xy=(i, mean_val),\n",
    "                            xytext=(0, 3),\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom', fontsize=10)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'No Sklearn Models\\nAvailable', \n",
    "                ha='center', va='center', transform=ax3.transAxes, fontsize=14)\n",
    "        ax3.set_title('Cross-Validation Scores (Sklearn Models)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Model complexity (Parameters for Keras) with better formatting\n",
    "    if len(successful_keras) > 0 and 'Parameters' in successful_keras.columns:\n",
    "        bars4 = ax4.bar(successful_keras['Method'], successful_keras['Parameters'], color='lightblue')\n",
    "        ax4.set_title('Model Complexity (Neural Networks)', fontsize=14, fontweight='bold')\n",
    "        ax4.set_ylabel('Number of Parameters', fontsize=12)\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format y-axis for better readability\n",
    "        ax4.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K' if x >= 1000 else f'{int(x)}'))\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, param_count in zip(bars4, successful_keras['Parameters']):\n",
    "            height = bar.get_height()\n",
    "            label = f'{param_count/1000:.1f}K' if param_count >= 1000 else f'{param_count}'\n",
    "            ax4.annotate(label,\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No Keras Models\\nAvailable', \n",
    "                ha='center', va='center', transform=ax4.transAxes, fontsize=14)\n",
    "        ax4.set_title('Model Complexity (Neural Networks)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add legend to distinguish model types\n",
    "    if len(successful_sklearn) > 0 and len(successful_keras) > 0:\n",
    "        legend_elements = [\n",
    "            plt.Rectangle((0,0),1,1, facecolor='lightcoral', label='Sklearn Models'),\n",
    "            plt.Rectangle((0,0),1,1, facecolor='lightblue', label='Keras Models')\n",
    "        ]\n",
    "        fig.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=2)\n",
    "    \n",
    "    plt.suptitle('Classification Performance Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n VISUALIZATION INSIGHTS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if len(all_accuracies) > 0:\n",
    "        best_acc_idx = np.argmax(all_accuracies)\n",
    "        fastest_idx = np.argmin(all_train_times) if len(all_train_times) > 0 else 0\n",
    "        \n",
    "        print(f\" Best Accuracy: {all_methods[best_acc_idx]} ({all_accuracies[best_acc_idx]:.4f})\")\n",
    "        if len(all_train_times) > 0:\n",
    "            print(f\" Fastest Training: {all_methods[fastest_idx]} ({all_train_times[fastest_idx]:.2f}s)\")\n",
    "        print(f\" Average Accuracy: {np.mean(all_accuracies):.4f}\")\n",
    "        if len(all_train_times) > 0:\n",
    "            print(f\"  Average Training Time: {np.mean(all_train_times):.2f}s\")\n",
    "        \n",
    "        if len(successful_sklearn) > 0:\n",
    "            sklearn_avg = successful_sklearn['Accuracy'].mean()\n",
    "            print(f\" Sklearn Average: {sklearn_avg:.4f}\")\n",
    "        \n",
    "        if len(successful_keras) > 0:\n",
    "            keras_avg = successful_keras['Accuracy'].mean()\n",
    "            print(f\" Keras Average: {keras_avg:.4f}\")\n",
    "    else:\n",
    "        print(\"  No accuracy data available to analyze\")\n",
    "\n",
    "else:\n",
    "    print(\" No successful models to visualize\")\n",
    "    print(\"Please check your model configurations and training process\")\n",
    "    \n",
    "    # Additional debugging information\n",
    "    print(\"\\nðŸ” Debugging Information:\")\n",
    "    if len(sklearn_df) > 0:\n",
    "        print(\"Sklearn DataFrame contains:\")\n",
    "        for col in sklearn_df.columns:\n",
    "            non_null_count = sklearn_df[col].count()\n",
    "            print(f\"  â€¢ {col}: {non_null_count} non-null values\")\n",
    "    \n",
    "    if len(keras_df) > 0:\n",
    "        print(\"Keras DataFrame contains:\")\n",
    "        for col in keras_df.columns:\n",
    "            non_null_count = keras_df[col].count()\n",
    "            print(f\"  â€¢ {col}: {non_null_count} non-null values\")\n",
    "    \n",
    "    # Check for error messages\n",
    "    if 'Error' in sklearn_df.columns:\n",
    "        errors = sklearn_df['Error'].dropna()\n",
    "        if len(errors) > 0:\n",
    "            print(\"\\nSklearn Errors:\")\n",
    "            for idx, error in errors.items():\n",
    "                print(f\"  â€¢ Row {idx}: {error}\")\n",
    "    \n",
    "    if 'Error' in keras_df.columns:\n",
    "        errors = keras_df['Error'].dropna()\n",
    "        if len(errors) > 0:\n",
    "            print(\"\\nKeras Errors:\")\n",
    "            for idx, error in errors.items():\n",
    "                print(f\"  â€¢ Row {idx}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43342feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(models_dict, model_type=\"Sklearn\"):\n",
    "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "    n_models = len(models_dict)\n",
    "    if n_models == 0:\n",
    "        return\n",
    "    \n",
    "    cols = min(3, n_models)\n",
    "    rows = (n_models + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    \n",
    "    # Fix axis handling for consistent indexing\n",
    "    if n_models == 1:\n",
    "        axes = [[axes]]  # Wrap in nested list for consistent [row][col] access\n",
    "    elif rows == 1:\n",
    "        axes = [axes]    # Wrap single row in list\n",
    "    elif cols == 1:\n",
    "        axes = [[ax] for ax in axes]  # Wrap single column\n",
    "    \n",
    "    for idx, (name, model_data) in enumerate(models_dict.items()):\n",
    "        row, col = idx // cols, idx % cols\n",
    "        ax = axes[row][col]  # Consistent access pattern\n",
    "        \n",
    "        # Get true labels and predictions\n",
    "        y_true = y_test_encoded if 'sklearn' in model_type.lower() else y_test_encoded\n",
    "        y_pred = model_data['predictions']\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Plot\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        ax.set_title(f'{name}\\nAccuracy: {model_data[\"results\"][\"accuracy\"]:.3f}')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_models, rows * cols):\n",
    "        row, col = idx // cols, idx % cols\n",
    "        ax = axes[row][col]\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'{model_type} Models - Confusion Matrices', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd982de",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95653e13",
   "metadata": {},
   "source": [
    "### Sklearn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e162b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(sklearn_models):\n",
    "    \"\"\"Analyze feature importance for models that support it\"\"\"\n",
    "    print(\" Feature Importance Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for name, model_data in sklearn_models.items():\n",
    "        classifier = model_data['classifier']\n",
    "        \n",
    "        try:\n",
    "            importance = classifier.get_feature_importance()\n",
    "            \n",
    "            # Get top 20 most important features\n",
    "            top_indices = np.argsort(importance)[-20:]\n",
    "            top_importance = importance[top_indices]\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(range(len(top_importance)), top_importance)\n",
    "            plt.title(f'Top 20 Feature Importance - {name}')\n",
    "            plt.xlabel('Importance')\n",
    "            plt.ylabel('Feature Index')\n",
    "            plt.yticks(range(len(top_importance)), top_indices)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Max importance: {np.max(importance):.6f}\")\n",
    "            print(f\"  Mean importance: {np.mean(importance):.6f}\")\n",
    "            print(f\"  Std importance: {np.std(importance):.6f}\")\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(f\"\\n{name}: {e}\")\n",
    "\n",
    "if len(sklearn_models) > 0:\n",
    "    analyze_feature_importance(sklearn_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158dce8",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80354b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_histories(keras_models):\n",
    "    \"\"\"Plot training histories for neural networks\"\"\"\n",
    "    if len(keras_models) == 0:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(len(keras_models), 2, figsize=(15, 5*len(keras_models)))\n",
    "    if len(keras_models) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, (name, model_data) in enumerate(keras_models.items()):\n",
    "        history = model_data['history']\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[idx, 0].plot(history.history['loss'], label='Train Loss', color='blue')\n",
    "        if 'val_loss' in history.history:\n",
    "            axes[idx, 0].plot(history.history['val_loss'], label='Val Loss', color='red')\n",
    "        axes[idx, 0].set_title(f'{name} - Loss')\n",
    "        axes[idx, 0].set_xlabel('Epoch')\n",
    "        axes[idx, 0].set_ylabel('Loss')\n",
    "        axes[idx, 0].legend()\n",
    "        axes[idx, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot accuracy\n",
    "        axes[idx, 1].plot(history.history['accuracy'], label='Train Accuracy', color='blue')\n",
    "        if 'val_accuracy' in history.history:\n",
    "            axes[idx, 1].plot(history.history['val_accuracy'], label='Val Accuracy', color='red')\n",
    "        axes[idx, 1].set_title(f'{name} - Accuracy')\n",
    "        axes[idx, 1].set_xlabel('Epoch')\n",
    "        axes[idx, 1].set_ylabel('Accuracy')\n",
    "        axes[idx, 1].legend()\n",
    "        axes[idx, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if len(keras_models) > 0:\n",
    "    plot_training_histories(keras_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11c82b",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n CLASSIFICATION ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find best performers\n",
    "all_results = []\n",
    "if len(successful_sklearn) > 0:\n",
    "    for _, row in successful_sklearn.iterrows():\n",
    "        all_results.append({\n",
    "            'Method': row['Method'],\n",
    "            'Type': 'Sklearn',\n",
    "            'Accuracy': row['Accuracy'],\n",
    "            'Train Time': row['Train Time (s)'],\n",
    "            'CV Score': row.get('CV Mean', 0)\n",
    "        })\n",
    "\n",
    "if len(successful_keras) > 0:\n",
    "    for _, row in successful_keras.iterrows():\n",
    "        all_results.append({\n",
    "            'Method': row['Method'],\n",
    "            'Type': 'Keras',\n",
    "            'Accuracy': row['Accuracy'],\n",
    "            'Train Time': row['Train Time (s)'],\n",
    "            'CV Score': 0  # Keras doesn't have CV in this analysis\n",
    "        })\n",
    "\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    best_accuracy = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "    fastest_training = results_df.loc[results_df['Train Time'].idxmin()]\n",
    "    \n",
    "    print(f\"\\n Performance Insights:\")\n",
    "    print(f\"   â€¢ Best Accuracy: {best_accuracy['Method']} ({best_accuracy['Accuracy']:.4f})\")\n",
    "    print(f\"   â€¢ Fastest Training: {fastest_training['Method']} ({fastest_training['Train Time']:.2f}s)\")\n",
    "    \n",
    "    sklearn_count = len(successful_sklearn)\n",
    "    keras_count = len(successful_keras)\n",
    "    \n",
    "    print(f\"\\n Model Statistics:\")\n",
    "    print(f\"   â€¢ Successful Sklearn models: {sklearn_count}\")\n",
    "    print(f\"   â€¢ Successful Keras models: {keras_count}\")\n",
    "    print(f\"   â€¢ Dataset size: {subset_size} samples\")\n",
    "    print(f\"   â€¢ Embedding dimension: {X_train_embeddings.shape[1]}\")\n",
    "    \n",
    "    print(f\"\\n Key Insights:\")\n",
    "    if sklearn_count > 0:\n",
    "        avg_sklearn_acc = successful_sklearn['Accuracy'].mean()\n",
    "        print(f\"   â€¢ Average Sklearn accuracy: {avg_sklearn_acc:.4f}\")\n",
    "    \n",
    "    if keras_count > 0:\n",
    "        avg_keras_acc = successful_keras['Accuracy'].mean()\n",
    "        print(f\"   â€¢ Average Keras accuracy: {avg_keras_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\n Recommendations:\")\n",
    "    print(f\"   â€¢ For speed: Use {fastest_training['Method']}\")\n",
    "    print(f\"   â€¢ For accuracy: Use {best_accuracy['Method']}\")\n",
    "\n",
    "else:\n",
    "    print(\"No successful models to analyze.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
